{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2.3 方策反復法の実装\n",
    "\n",
    "前節では迷路内をAgentがランダムに行動する方策を実装した。  \n",
    "これから、Agentが一直線にゴールへ向かうように方策を学習させる。\n",
    "この方法は大きく二つの方法がある。\n",
    "\n",
    "## 方策反復法\n",
    "うまくいったケースの行動を重要視する作戦\n",
    "\n",
    "具体的な実装として、以下のような方策勾配法がある\n",
    "\n",
    "\n",
    "$$\n",
    "\\theta_{s_i,a_j} \\leftarrow \\theta_{s_i,a_j} + \\eta \\cdot \\Delta \\theta_{s, a_j} \\\\ \\Delta \\theta_{s, a_j} = \\{ N(s_i, a_j) - P(s_i, a_j) N (s_i, a) \\}  / T \n",
    "$$\n",
    "\n",
    "(softmax関数を使用した確率への変換とREINFORCEアルゴリズムに従うとこのような更新式が得られる)\n",
    "\n",
    "## 価値反復法\n",
    "ゴール以外の状態にも価値をつけてあげる\n",
    "\n",
    "具体的な実装として、以下のようなQ学習がある\n",
    "\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\eta \\times (R_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n",
    "$$\n",
    "\n",
    "\n",
    "ここでは方策反復法を実装する。\n",
    "前節では単純にθを割合に変換して方策を求める関数を実装したが、本節ではθを変換する際にsoftmax関数を使用する。\n",
    "ここでsoftmax関数を使用したのは、Θの更新式によってΘが負の値になっても方策を導出できるようにするため。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[0.         0.5        0.5        0.        ]\n [0.         0.5        0.         0.5       ]\n [0.         0.         0.5        0.5       ]\n [0.33333333 0.33333333 0.33333333 0.        ]\n [0.         0.         0.5        0.5       ]\n [1.         0.         0.         0.        ]\n [1.         0.         0.         0.        ]\n [0.5        0.5        0.         0.        ]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_convert_into_pi_from_theta(theta):\n",
    "    \"\"\"単純に割合を計算する\"\"\"\n",
    "    [m,n] = theta.shape\n",
    "    pi = np.zeros((m,n))\n",
    "    for i in range(0,m):\n",
    "        pi[i,:] = theta[i,:] / np.nansum(theta[i,:])\n",
    "        \n",
    "    pi = np.nan_to_num(pi)\n",
    "    return pi\n",
    "    \n",
    "    \n",
    "# 前節で用いた`simple_convert_into_pi_from_theta`と比較しながら読んで欲しい。\n",
    "def softmax_convert_into_pi_from_theta(theta):\n",
    "    \"\"\"ソフトマックス関数で割合を計算する\"\"\"\n",
    "    beta = 1.0\n",
    "    [m,n] = theta.shape\n",
    "    pi = np.zeros((m,n))\n",
    "    \n",
    "    exp_theta = np.exp(beta * theta)\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        pi[i,:] = exp_theta[i,:] / np.nansum(exp_theta[i,:])\n",
    "        \n",
    "    pi = np.nan_to_num(pi)\n",
    "    return pi\n",
    "    \n",
    "    \n",
    "theta_0 = np.array([\n",
    "    [np.nan, 1, 1, np.nan],\n",
    "    [np.nan, 1, np.nan, 1],\n",
    "    [np.nan, np.nan, 1, 1],\n",
    "    [1, 1, 1, np.nan],\n",
    "    [np.nan, np.nan, 1, 1],\n",
    "    [1, np.nan, np.nan, np.nan],\n",
    "    [1, np.nan, np.nan, np.nan],\n",
    "    [1, 1, np.nan, np.nan],\n",
    "])    \n",
    "\n",
    "pi_0 = softmax_convert_into_pi_from_theta(theta_0)\n",
    "print(pi_0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "初期状態では前節と同じ結果になるが、学習に従いθの値が変わると、関数`softmax_convert_into_pi_from_theta`の計算結果は単純な割合計算とは異なる結果となる。\n",
    "\n",
    "続いて、softmax関数による$\\pi_{\\theta} (s,a)$に従ってAgentを行動させる関数を定義する。\n",
    "θの更新時に必要な情報を取得するため、状態だけでなくその時採用した行動も取得する。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_action_and_next_s(pi,s):\n",
    "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "    # 二重配列piのs番目の配列の要素(確率)に従って、directionが選択される\n",
    "    next_direction = np.random.choice(direction, p=pi[s, :])\n",
    "\n",
    "    if next_direction == \"up\":\n",
    "        action = 0\n",
    "        s_next = s - 3\n",
    "    elif next_direction == \"right\":\n",
    "        action = 1\n",
    "        s_next = s + 1\n",
    "    elif next_direction == \"down\":\n",
    "        action = 2\n",
    "        s_next = s + 3\n",
    "    else:\n",
    "        action = 3\n",
    "        s_next = s - 1\n",
    "\n",
    "    return [action, s_next]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ゴールにたどり着くまでAgentを移動させ続ける関数を定義する。\n",
    "Agentの状態と、その状態で採用した行動の組みの配列を返す。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def goal_maze_ret_s_a(pi):\n",
    "    s = 0\n",
    "    s_a_history = [[0, np.nan]]\n",
    "    \n",
    "    while True :\n",
    "        [action, next_s] = get_action_and_next_s(pi,s)\n",
    "        # -1は現在の状態(=一番最後の状態)を指す\n",
    "        # 必要なら板書で説明する\n",
    "        s_a_history[-1][1] = action\n",
    "    \n",
    "        s_a_history.append([next_s, np.nan])\n",
    "        \n",
    "        # ゴールなら終了\n",
    "        if next_s == 8:\n",
    "            break\n",
    "        else:\n",
    "            s = next_s\n",
    "            \n",
    "    return s_a_history\n",
    "\n",
    "\n",
    "s_a_history = goal_maze_ret_s_a(pi_0)\n",
    "print(s_a_history)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}